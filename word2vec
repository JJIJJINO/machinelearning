import pandas as pd
import matplotlib.pyplot as plt
import urllib.request
import konlpy
from gensim.models.word2vec import Word2Vec
from konlpy.tag import Okt
import re
from tqdm import tqdm

------------------------------------------------------------------------------------------------------------------------------------
news_data = pd.read_excel('C:/Users/dragew/Desktop/과제 3/네이버뉴스_본문_100개_esg_2021-11-22_15시11분.xlsx')

------------------------------------------------------------------------------------------------------------------------------------
news_data[:5]

------------------------------------------------------------------------------------------------------------------------------------
print(len(news_data)) #데이터 갯수

------------------------------------------------------------------------------------------------------------------------------------

#음.. 데이터 전처리 한글, 영어 빼고 삭제
news_data['text']=news_data['text'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 a-z A-Z ]"," ")

------------------------------------------------------------------------------------------------------------------------------------
#음.... 불용어 목록 더 추가해야함
stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다','을','이다',
             '에서', '되다', '돼다', '인', '를', '대한', '로', '인', '듬', '하고', '않다', '가다', '또','받다',
            '때문', '이렇다', '등', '위', '고', '위해', '있다', '어떻다', '있다', '말', '것', '게', '수', '아니다'
            , '년', '그', '우리', '다', '보다', '만', '그렇다', '적', '더', '같다', '까지', '없다', '생각', '앵커'
            ,'일','경우','지금','답변','그런데','나오다', '최근', '요', '그래서', '많다', '해', '기자', '거', '부터'
            , '한국', '에도', '뿐', '및', '계', '곳', '중', '대다', '개', '때', '뿐','제','만들다','안','죠']

------------------------------------------------------------------------------------------------------------------------------------
okt = Okt()

tokenized_data = []
for sentence in tqdm(news_data['text']):
    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화
    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거
    tokenized_data.append(stopwords_removed_sentence)
    
------------------------------------------------------------------------------------------------------------------------------------
tokenized_data[:10]

------------------------------------------------------------------------------------------------------------------------------------
print('기사의 최대 길이 :',max(len(l) for l in tokenized_data))
print('기사의 평균 길이 :',sum(map(len, tokenized_data))/len(tokenized_data))
plt.hist([len(s) for s in tokenized_data], bins=50)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()

------------------------------------------------------------------------------------------------------------------------------------
from gensim.models import Word2Vec
model = Word2Vec(sentences = tokenized_data, vector_size = 60000, window = 6, min_count = 5, workers = 10, sg = 0)
#size = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.
#window = 컨텍스트 윈도우 크기
#min_count = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)
#workers = 학습을 위한 프로세스 수
#sg = 0은 CBOW, 1은 Skip-gram.

------------------------------------------------------------------------------------------------------------------------------------
print(model.wv.most_similar("ESG"))
